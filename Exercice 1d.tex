\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, bm}

\title{ex1d}
\author{LI QINGYUE}
\date{October 2023}

\begin{document}

\section*{0.4 Exercice 1d}

1. \textbf{Définition du modèle}:
\[ Y_{ij} = a_i + b_i X_{ij} + \beta X_{ij} + \epsilon_{ij} \]
où:
\begin{itemize}
    \item \( Y_{ij} \) est la réponse observée pour l'individu \( i \) à l'occasion \( j \).
    \item \( a_i \) est l'interception aléatoire pour l'individu \( i \).
    \item \( b_i \) est la pente aléatoire pour l'individu \( i \).
    \item \( \beta \) est la pente fixe associée à la variable explicative \( X \).
    \item \( X_{ij} \) est la variable explicative pour l'individu \( i \) à l'occasion \( j \).
    \item \( \epsilon_{ij} \) est l'erreur résiduelle pour l'individu \( i \) à l'occasion \( j \).
\end{itemize}

2. \textbf{Hypothèses sur les composants aléatoires}:
\begin{align*}
    a_i &\sim N(4, \tau^2) \\
    b_i &\sim N(0, \psi^2) \\
    \text{Cov}(\epsilon_{ij}) &= \sigma^2 \bm{V}
\end{align*}
où \( \bm{V} \) est une matrice qui définit une corrélation de type AR(1) entre les erreurs pour un même individu \( i \).

\textbf{Solution}:
Nous revoyons d'abord le modèle donné dans la question:

\[
Y_{ij} = a_i + b_i X_{ij} + \beta X_{ij} + \epsilon_{ij}
\]

i) Pour séparer le modèle en deux composantes, considérons d'abord la partie de régression linéaire:

La partie de régression linéaire contient les éléments déterministes ou systématiques du modèle. Elle représente la tendance générale des données \(X_{ij}\). Dans ce modèle, la partie linéaire est:

\[
a_i + b_i X_{ij} + \beta X_{ij}
\]

Nous pouvons combiner les termes associés à \(X_{ij}\):

\[
a_i + (b_i + \beta) X_{ij}
\]

Ceci est la partie de régression linéaire.

Ensuite, considérons la partie d'erreur \(W_{ij}\):

\[
W_{ij} = \epsilon_{ij}
\]

L'erreur \(W_{ij}\) reflète la variabilité que la partie linéaire ne peut pas expliquer. Elle est une combinaison des erreurs individuelles et globales, mais dans ce modèle, elle est seulement représentée par \(\epsilon_{ij}\).

En conclusion, le modèle peut être séparé comme:

\[
Y_{ij} = a_i + (b_i + \beta) X_{ij} + W_{ij}
\]

où:
\begin{itemize}
    \item \(a_i + (b_i + \beta) X_{ij}\) est la partie de régression linéaire.
    \item \(W_{ij}\) est la partie d'erreur.
\end{itemize}

Ceci est cohérent avec la façon dont nous séparons les modèles linéaires en classe (une partie déterministe et une partie d'erreur).

ii) Pour la variance de \( Y_{ij} \), nous considérons la somme des variances des trois effets aléatoires (intercept, pente, et l'erreur) :

\begin{equation}
\text{Var}(Y_{ij}) = \text{Var}(a_i) + \text{Var}(b_i X_{ij}) + \text{Var}(\epsilon_{ij})
\end{equation}

En utilisant les informations fournies :

\begin{equation}
\text{Var}(Y_{ij}) = \tau^2 + \psi^2 X_{ij}^2 + \sigma^2
\end{equation}

Cette variance sera l'élément diagonal de la matrice de covariance.
La covariance entre deux observations de \( Y \) à deux moments différents \( j \) et \( k \) est donnée par :

\begin{equation}
\text{Cov}(Y_{ij}, Y_{ik}) = \text{Cov}(a_i + b_i X_{ij} + \epsilon_{ij}, a_i + b_i X_{ik} + \epsilon_{ik})
\end{equation}

En tenant compte des effets aléatoires \( a_i \) et \( b_i \) qui sont indépendants, et de la structure de covariance entre les erreurs \( \epsilon_{ij} \) qui est définie par la matrice \( V \) :

\begin{equation}
\text{Cov}(Y_{ij}, Y_{ik}) = \tau^2 + \psi^2 X_{ij} X_{ik} + \sigma^2 \rho^{|j-k|}
\end{equation}

où \( \rho^{|j-k|} \) est défini par la structure AR(1) de la matrice \( V \).

Nous pouvons maintenant construire la matrice de covariance 5x5 \( \text{Cov}(Y_i) \) pour chaque individu \( i \) en utilisant les formules de variance et de covariance ci-dessus.


\end{document}
